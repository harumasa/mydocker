{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chapter 16 - Modeling Sequential Data Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('movie_data.csv.gz') as f_in, open('movie_data.csv', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 16.1. Introducing sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.1.1. Modeling sequential data: Order matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.1.2. Understanding the different categories of sequence modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 16.2. Recurrent neural networks for modeling sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.2.1. Understanding the structure and flow of a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.2.2. Computing activations in an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.2.3. The challenges of learning long-range interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.2.4. Long short-term memory units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 16.3. Implementing a multilayer RNN for sequence modeling in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.3.1. Performing sentiment analysis of IMDb movie reviews using multilayer RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 16.3.1.1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1  OK... so... I really like Kris Kristofferson a...          0\n",
      "2  ***SPOILER*** Do not read this, if you think a...          0\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @Readers: PLEASE IGNORE THIS CELL\n",
    "##\n",
    "## This cell is meant to shrink the\n",
    "## dataset when this notebook is run \n",
    "## on the Travis Continuous Integration\n",
    "## platform to test the code as well as\n",
    "## speeding up the run using a smaller\n",
    "## dataset for debugging\n",
    "\n",
    "import os\n",
    "\n",
    "if 'TRAVIS' in os.environ:\n",
    "    df = pd.read_csv('movie_data.csv', encoding='utf-8', nrows=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:06:46\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing the data:\n",
    "## Separate words and \n",
    "## count each word's occurrence\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Counting words occurences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:03\n"
     ]
    }
   ],
   "source": [
    "## Create a mapping:\n",
    "## Map each unique word to an integer\n",
    "\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define fixed-length sequences:\n",
    "## Use the last 200 elements of each sequence\n",
    "## if sequence length < 200: left-pad with zeros\n",
    "\n",
    "sequence_length = 200  ## sequence length (or T in our formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x= x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @Readers: PLEASE IGNORE THIS CELL\n",
    "##\n",
    "## This cell is meant to shrink the\n",
    "## dataset when this notebook is run \n",
    "## on the Travis Continuous Integration\n",
    "## platform to test the code as well as\n",
    "## speeding up the run using a smaller\n",
    "## dataset for debugging\n",
    "\n",
    "if 'TRAVIS' in os.environ:\n",
    "    X_train = sequences[:250, :]\n",
    "    y_train = df.loc[:250, 'sentiment'].values\n",
    "    X_test = sequences[250:500, :]\n",
    "    y_test = df.loc[250:500, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 16.3.1.2. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 16.3.1.3. Building the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size   ## number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                                        shape=(self.batch_size, self.seq_len),\n",
    "                                        name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                                        shape=(self.batch_size),\n",
    "                                        name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                                               name='tf_keepprob')\n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval=-1, maxval=1),\n",
    "                    name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name='embeded_x')\n",
    "\n",
    "        ## Define LSTM cell and stack them together\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.DropoutWrapper(\n",
    "                   tf.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
    "                   output_keep_prob=tf_keepprob)\n",
    "                 for i in range(self.num_layers)])\n",
    "\n",
    "        ## Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "                 self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                 cells, embed_x,\n",
    "                 initial_state=self.initial_state)\n",
    "        ## Note: lstm_outputs shape: \n",
    "        ##  [batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "\n",
    "        ## Apply a FC layer after on top of RNN output:\n",
    "        logits = tf.layers.dense(\n",
    "                 inputs=lstm_outputs[:, -1],\n",
    "                 units=1, activation=None,\n",
    "                 name='logits')\n",
    "\n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "\n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba),\n",
    "                                         tf.int32,\n",
    "                 name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "\n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                 tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                 labels=tf_y, logits=logits),\n",
    "                 name='cost')\n",
    "\n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "\n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                            X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['cost:0', 'train_op',\n",
    "                             self.final_state],\n",
    "                            feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, num_epochs,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    iteration +=1\n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/sentiment-%d.ckpt\" % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0' : batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                preds.append(pred)\n",
    "\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### 16.3.1.3.1. Step 1: Defining multilayer RNN cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### 16.3.1.3.2. Step 2: Defining the initial states for the RNN cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### 16.3.1.3.3. Step 3: Creating the recurrent neural network using the RNN cells and their states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output   >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Identity_4:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Identity_5:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "## Train:\n",
    "\n",
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words,\n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256,\n",
    "                   lstm_size=128,\n",
    "                   num_layers=1,\n",
    "                   batch_size=100,\n",
    "                   learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.67923\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.57470\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.66716\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.57895\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.58712\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.56508\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 0.43767\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.60172\n",
      "Epoch: 1/40 Iteration: 180 | Train loss: 0.57916\n",
      "Epoch: 1/40 Iteration: 200 | Train loss: 0.58948\n",
      "Epoch: 1/40 Iteration: 220 | Train loss: 0.50243\n",
      "Epoch: 1/40 Iteration: 240 | Train loss: 0.46655\n",
      "Epoch: 2/40 Iteration: 260 | Train loss: 0.50657\n",
      "Epoch: 2/40 Iteration: 280 | Train loss: 0.36778\n",
      "Epoch: 2/40 Iteration: 300 | Train loss: 0.43959\n",
      "Epoch: 2/40 Iteration: 320 | Train loss: 0.30079\n",
      "Epoch: 2/40 Iteration: 340 | Train loss: 0.41402\n",
      "Epoch: 2/40 Iteration: 360 | Train loss: 0.26500\n",
      "Epoch: 2/40 Iteration: 380 | Train loss: 0.37391\n",
      "Epoch: 2/40 Iteration: 400 | Train loss: 0.33173\n",
      "Epoch: 2/40 Iteration: 420 | Train loss: 0.40439\n",
      "Epoch: 2/40 Iteration: 440 | Train loss: 0.33566\n",
      "Epoch: 2/40 Iteration: 460 | Train loss: 0.46694\n",
      "Epoch: 2/40 Iteration: 480 | Train loss: 0.28473\n",
      "Epoch: 2/40 Iteration: 500 | Train loss: 0.21732\n",
      "Epoch: 3/40 Iteration: 520 | Train loss: 0.29341\n",
      "Epoch: 3/40 Iteration: 540 | Train loss: 0.23757\n",
      "Epoch: 3/40 Iteration: 560 | Train loss: 0.38657\n",
      "Epoch: 3/40 Iteration: 580 | Train loss: 0.20655\n",
      "Epoch: 3/40 Iteration: 600 | Train loss: 0.26394\n",
      "Epoch: 3/40 Iteration: 620 | Train loss: 0.21498\n",
      "Epoch: 3/40 Iteration: 640 | Train loss: 0.17437\n",
      "Epoch: 3/40 Iteration: 660 | Train loss: 0.17185\n",
      "Epoch: 3/40 Iteration: 680 | Train loss: 0.28568\n",
      "Epoch: 3/40 Iteration: 700 | Train loss: 0.20980\n",
      "Epoch: 3/40 Iteration: 720 | Train loss: 0.18728\n",
      "Epoch: 3/40 Iteration: 740 | Train loss: 0.27404\n",
      "Epoch: 4/40 Iteration: 760 | Train loss: 0.25010\n",
      "Epoch: 4/40 Iteration: 780 | Train loss: 0.17021\n",
      "Epoch: 4/40 Iteration: 800 | Train loss: 0.13577\n",
      "Epoch: 4/40 Iteration: 820 | Train loss: 0.30503\n",
      "Epoch: 4/40 Iteration: 840 | Train loss: 0.10306\n",
      "Epoch: 4/40 Iteration: 860 | Train loss: 0.12588\n",
      "Epoch: 4/40 Iteration: 880 | Train loss: 0.38281\n",
      "Epoch: 4/40 Iteration: 900 | Train loss: 0.26744\n",
      "Epoch: 4/40 Iteration: 920 | Train loss: 0.20751\n",
      "Epoch: 4/40 Iteration: 940 | Train loss: 0.29139\n",
      "Epoch: 4/40 Iteration: 960 | Train loss: 0.35780\n",
      "Epoch: 4/40 Iteration: 980 | Train loss: 0.17297\n",
      "Epoch: 4/40 Iteration: 1000 | Train loss: 0.17719\n",
      "Epoch: 5/40 Iteration: 1020 | Train loss: 0.14705\n",
      "Epoch: 5/40 Iteration: 1040 | Train loss: 0.13299\n",
      "Epoch: 5/40 Iteration: 1060 | Train loss: 0.23908\n",
      "Epoch: 5/40 Iteration: 1080 | Train loss: 0.15350\n",
      "Epoch: 5/40 Iteration: 1100 | Train loss: 0.22812\n",
      "Epoch: 5/40 Iteration: 1120 | Train loss: 0.28173\n",
      "Epoch: 5/40 Iteration: 1140 | Train loss: 0.16198\n",
      "Epoch: 5/40 Iteration: 1160 | Train loss: 0.09672\n",
      "Epoch: 5/40 Iteration: 1180 | Train loss: 0.14464\n",
      "Epoch: 5/40 Iteration: 1200 | Train loss: 0.07256\n",
      "Epoch: 5/40 Iteration: 1220 | Train loss: 0.09005\n",
      "Epoch: 5/40 Iteration: 1240 | Train loss: 0.13231\n",
      "Epoch: 6/40 Iteration: 1260 | Train loss: 0.17961\n",
      "Epoch: 6/40 Iteration: 1280 | Train loss: 0.05084\n",
      "Epoch: 6/40 Iteration: 1300 | Train loss: 0.09666\n",
      "Epoch: 6/40 Iteration: 1320 | Train loss: 0.18572\n",
      "Epoch: 6/40 Iteration: 1340 | Train loss: 0.09667\n",
      "Epoch: 6/40 Iteration: 1360 | Train loss: 0.04624\n",
      "Epoch: 6/40 Iteration: 1380 | Train loss: 0.24136\n",
      "Epoch: 6/40 Iteration: 1400 | Train loss: 0.24489\n",
      "Epoch: 6/40 Iteration: 1420 | Train loss: 0.10339\n",
      "Epoch: 6/40 Iteration: 1440 | Train loss: 0.14567\n",
      "Epoch: 6/40 Iteration: 1460 | Train loss: 0.07982\n",
      "Epoch: 6/40 Iteration: 1480 | Train loss: 0.09725\n",
      "Epoch: 6/40 Iteration: 1500 | Train loss: 0.13430\n",
      "Epoch: 7/40 Iteration: 1520 | Train loss: 0.15087\n",
      "Epoch: 7/40 Iteration: 1540 | Train loss: 0.04741\n",
      "Epoch: 7/40 Iteration: 1560 | Train loss: 0.25341\n",
      "Epoch: 7/40 Iteration: 1580 | Train loss: 0.07749\n",
      "Epoch: 7/40 Iteration: 1600 | Train loss: 0.10659\n",
      "Epoch: 7/40 Iteration: 1620 | Train loss: 0.10924\n",
      "Epoch: 7/40 Iteration: 1640 | Train loss: 0.03258\n",
      "Epoch: 7/40 Iteration: 1660 | Train loss: 0.06554\n",
      "Epoch: 7/40 Iteration: 1680 | Train loss: 0.13592\n",
      "Epoch: 7/40 Iteration: 1700 | Train loss: 0.09461\n",
      "Epoch: 7/40 Iteration: 1720 | Train loss: 0.14614\n",
      "Epoch: 7/40 Iteration: 1740 | Train loss: 0.06434\n",
      "Epoch: 8/40 Iteration: 1760 | Train loss: 0.09961\n",
      "Epoch: 8/40 Iteration: 1780 | Train loss: 0.04198\n",
      "Epoch: 8/40 Iteration: 1800 | Train loss: 0.04453\n",
      "Epoch: 8/40 Iteration: 1820 | Train loss: 0.14429\n",
      "Epoch: 8/40 Iteration: 1840 | Train loss: 0.12635\n",
      "Epoch: 8/40 Iteration: 1860 | Train loss: 0.09239\n",
      "Epoch: 8/40 Iteration: 1880 | Train loss: 0.11393\n",
      "Epoch: 8/40 Iteration: 1900 | Train loss: 0.08583\n",
      "Epoch: 8/40 Iteration: 1920 | Train loss: 0.06938\n",
      "Epoch: 8/40 Iteration: 1940 | Train loss: 0.07407\n",
      "Epoch: 8/40 Iteration: 1960 | Train loss: 0.09869\n",
      "Epoch: 8/40 Iteration: 1980 | Train loss: 0.05361\n",
      "Epoch: 8/40 Iteration: 2000 | Train loss: 0.02828\n",
      "Epoch: 9/40 Iteration: 2020 | Train loss: 0.07568\n",
      "Epoch: 9/40 Iteration: 2040 | Train loss: 0.01858\n",
      "Epoch: 9/40 Iteration: 2060 | Train loss: 0.07048\n",
      "Epoch: 9/40 Iteration: 2080 | Train loss: 0.03409\n",
      "Epoch: 9/40 Iteration: 2100 | Train loss: 0.04323\n",
      "Epoch: 9/40 Iteration: 2120 | Train loss: 0.09144\n",
      "Epoch: 9/40 Iteration: 2140 | Train loss: 0.03566\n",
      "Epoch: 9/40 Iteration: 2160 | Train loss: 0.06973\n",
      "Epoch: 9/40 Iteration: 2180 | Train loss: 0.03009\n",
      "Epoch: 9/40 Iteration: 2200 | Train loss: 0.01547\n",
      "Epoch: 9/40 Iteration: 2220 | Train loss: 0.00983\n",
      "Epoch: 9/40 Iteration: 2240 | Train loss: 0.03351\n",
      "Epoch: 10/40 Iteration: 2260 | Train loss: 0.05945\n",
      "Epoch: 10/40 Iteration: 2280 | Train loss: 0.01407\n",
      "Epoch: 10/40 Iteration: 2300 | Train loss: 0.01208\n",
      "Epoch: 10/40 Iteration: 2320 | Train loss: 0.05431\n",
      "Epoch: 10/40 Iteration: 2340 | Train loss: 0.01515\n",
      "Epoch: 10/40 Iteration: 2360 | Train loss: 0.01198\n",
      "Epoch: 10/40 Iteration: 2380 | Train loss: 0.08566\n",
      "Epoch: 10/40 Iteration: 2400 | Train loss: 0.13704\n",
      "Epoch: 10/40 Iteration: 2420 | Train loss: 0.04560\n",
      "Epoch: 10/40 Iteration: 2440 | Train loss: 0.03961\n",
      "Epoch: 10/40 Iteration: 2460 | Train loss: 0.01421\n",
      "Epoch: 10/40 Iteration: 2480 | Train loss: 0.03488\n",
      "Epoch: 10/40 Iteration: 2500 | Train loss: 0.01345\n",
      "Epoch: 11/40 Iteration: 2520 | Train loss: 0.03740\n",
      "Epoch: 11/40 Iteration: 2540 | Train loss: 0.00472\n",
      "Epoch: 11/40 Iteration: 2560 | Train loss: 0.01108\n",
      "Epoch: 11/40 Iteration: 2580 | Train loss: 0.00983\n",
      "Epoch: 11/40 Iteration: 2600 | Train loss: 0.00817\n",
      "Epoch: 11/40 Iteration: 2620 | Train loss: 0.05564\n",
      "Epoch: 11/40 Iteration: 2640 | Train loss: 0.00620\n",
      "Epoch: 11/40 Iteration: 2660 | Train loss: 0.03442\n",
      "Epoch: 11/40 Iteration: 2680 | Train loss: 0.00793\n",
      "Epoch: 11/40 Iteration: 2700 | Train loss: 0.00972\n",
      "Epoch: 11/40 Iteration: 2720 | Train loss: 0.00478\n",
      "Epoch: 11/40 Iteration: 2740 | Train loss: 0.01589\n",
      "Epoch: 12/40 Iteration: 2760 | Train loss: 0.02943\n",
      "Epoch: 12/40 Iteration: 2780 | Train loss: 0.02832\n",
      "Epoch: 12/40 Iteration: 2800 | Train loss: 0.01126\n",
      "Epoch: 12/40 Iteration: 2820 | Train loss: 0.00295\n",
      "Epoch: 12/40 Iteration: 2840 | Train loss: 0.01913\n",
      "Epoch: 12/40 Iteration: 2860 | Train loss: 0.00792\n",
      "Epoch: 12/40 Iteration: 2880 | Train loss: 0.04314\n",
      "Epoch: 12/40 Iteration: 2900 | Train loss: 0.00854\n",
      "Epoch: 12/40 Iteration: 2920 | Train loss: 0.02644\n",
      "Epoch: 12/40 Iteration: 2940 | Train loss: 0.02505\n",
      "Epoch: 12/40 Iteration: 2960 | Train loss: 0.04271\n",
      "Epoch: 12/40 Iteration: 2980 | Train loss: 0.00969\n",
      "Epoch: 12/40 Iteration: 3000 | Train loss: 0.01063\n",
      "Epoch: 13/40 Iteration: 3020 | Train loss: 0.01548\n",
      "Epoch: 13/40 Iteration: 3040 | Train loss: 0.03185\n",
      "Epoch: 13/40 Iteration: 3060 | Train loss: 0.02554\n",
      "Epoch: 13/40 Iteration: 3080 | Train loss: 0.02149\n",
      "Epoch: 13/40 Iteration: 3100 | Train loss: 0.04896\n",
      "Epoch: 13/40 Iteration: 3120 | Train loss: 0.06930\n",
      "Epoch: 13/40 Iteration: 3140 | Train loss: 0.00424\n",
      "Epoch: 13/40 Iteration: 3160 | Train loss: 0.00250\n",
      "Epoch: 13/40 Iteration: 3180 | Train loss: 0.00424\n",
      "Epoch: 13/40 Iteration: 3200 | Train loss: 0.06078\n",
      "Epoch: 13/40 Iteration: 3220 | Train loss: 0.02818\n",
      "Epoch: 13/40 Iteration: 3240 | Train loss: 0.02086\n",
      "Epoch: 14/40 Iteration: 3260 | Train loss: 0.02775\n",
      "Epoch: 14/40 Iteration: 3280 | Train loss: 0.01325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40 Iteration: 3300 | Train loss: 0.00300\n",
      "Epoch: 14/40 Iteration: 3320 | Train loss: 0.00567\n",
      "Epoch: 14/40 Iteration: 3340 | Train loss: 0.01712\n",
      "Epoch: 14/40 Iteration: 3360 | Train loss: 0.00161\n",
      "Epoch: 14/40 Iteration: 3380 | Train loss: 0.02407\n",
      "Epoch: 14/40 Iteration: 3400 | Train loss: 0.00663\n",
      "Epoch: 14/40 Iteration: 3420 | Train loss: 0.07528\n",
      "Epoch: 14/40 Iteration: 3440 | Train loss: 0.06457\n",
      "Epoch: 14/40 Iteration: 3460 | Train loss: 0.00516\n",
      "Epoch: 14/40 Iteration: 3480 | Train loss: 0.05171\n",
      "Epoch: 14/40 Iteration: 3500 | Train loss: 0.03853\n",
      "Epoch: 15/40 Iteration: 3520 | Train loss: 0.01082\n",
      "Epoch: 15/40 Iteration: 3540 | Train loss: 0.00735\n",
      "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00461\n",
      "Epoch: 15/40 Iteration: 3580 | Train loss: 0.00636\n",
      "Epoch: 15/40 Iteration: 3600 | Train loss: 0.00283\n",
      "Epoch: 15/40 Iteration: 3620 | Train loss: 0.03917\n",
      "Epoch: 15/40 Iteration: 3640 | Train loss: 0.00193\n",
      "Epoch: 15/40 Iteration: 3660 | Train loss: 0.00397\n",
      "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00191\n",
      "Epoch: 15/40 Iteration: 3700 | Train loss: 0.00232\n",
      "Epoch: 15/40 Iteration: 3720 | Train loss: 0.00119\n",
      "Epoch: 15/40 Iteration: 3740 | Train loss: 0.00679\n",
      "Epoch: 16/40 Iteration: 3760 | Train loss: 0.01259\n",
      "Epoch: 16/40 Iteration: 3780 | Train loss: 0.00782\n",
      "Epoch: 16/40 Iteration: 3800 | Train loss: 0.00155\n",
      "Epoch: 16/40 Iteration: 3820 | Train loss: 0.00274\n",
      "Epoch: 16/40 Iteration: 3840 | Train loss: 0.05664\n",
      "Epoch: 16/40 Iteration: 3860 | Train loss: 0.00523\n",
      "Epoch: 16/40 Iteration: 3880 | Train loss: 0.02607\n",
      "Epoch: 16/40 Iteration: 3900 | Train loss: 0.00419\n",
      "Epoch: 16/40 Iteration: 3920 | Train loss: 0.02610\n",
      "Epoch: 16/40 Iteration: 3940 | Train loss: 0.04983\n",
      "Epoch: 16/40 Iteration: 3960 | Train loss: 0.01055\n",
      "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00799\n",
      "Epoch: 16/40 Iteration: 4000 | Train loss: 0.01924\n",
      "Epoch: 17/40 Iteration: 4020 | Train loss: 0.03402\n",
      "Epoch: 17/40 Iteration: 4040 | Train loss: 0.00230\n",
      "Epoch: 17/40 Iteration: 4060 | Train loss: 0.00639\n",
      "Epoch: 17/40 Iteration: 4080 | Train loss: 0.00247\n",
      "Epoch: 17/40 Iteration: 4100 | Train loss: 0.00203\n",
      "Epoch: 17/40 Iteration: 4120 | Train loss: 0.07347\n",
      "Epoch: 17/40 Iteration: 4140 | Train loss: 0.01380\n",
      "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00476\n",
      "Epoch: 17/40 Iteration: 4180 | Train loss: 0.00239\n",
      "Epoch: 17/40 Iteration: 4200 | Train loss: 0.00475\n",
      "Epoch: 17/40 Iteration: 4220 | Train loss: 0.03173\n",
      "Epoch: 17/40 Iteration: 4240 | Train loss: 0.00506\n",
      "Epoch: 18/40 Iteration: 4260 | Train loss: 0.01634\n",
      "Epoch: 18/40 Iteration: 4280 | Train loss: 0.02384\n",
      "Epoch: 18/40 Iteration: 4300 | Train loss: 0.00900\n",
      "Epoch: 18/40 Iteration: 4320 | Train loss: 0.00050\n",
      "Epoch: 18/40 Iteration: 4340 | Train loss: 0.00586\n",
      "Epoch: 18/40 Iteration: 4360 | Train loss: 0.00261\n",
      "Epoch: 18/40 Iteration: 4380 | Train loss: 0.02824\n",
      "Epoch: 18/40 Iteration: 4400 | Train loss: 0.00720\n",
      "Epoch: 18/40 Iteration: 4420 | Train loss: 0.00855\n",
      "Epoch: 18/40 Iteration: 4440 | Train loss: 0.00354\n",
      "Epoch: 18/40 Iteration: 4460 | Train loss: 0.00775\n",
      "Epoch: 18/40 Iteration: 4480 | Train loss: 0.00273\n",
      "Epoch: 18/40 Iteration: 4500 | Train loss: 0.00343\n",
      "Epoch: 19/40 Iteration: 4520 | Train loss: 0.00066\n",
      "Epoch: 19/40 Iteration: 4540 | Train loss: 0.00099\n",
      "Epoch: 19/40 Iteration: 4560 | Train loss: 0.00284\n",
      "Epoch: 19/40 Iteration: 4580 | Train loss: 0.00225\n",
      "Epoch: 19/40 Iteration: 4600 | Train loss: 0.01444\n",
      "Epoch: 19/40 Iteration: 4620 | Train loss: 0.00706\n",
      "Epoch: 19/40 Iteration: 4640 | Train loss: 0.00122\n",
      "Epoch: 19/40 Iteration: 4660 | Train loss: 0.00246\n",
      "Epoch: 19/40 Iteration: 4680 | Train loss: 0.00112\n",
      "Epoch: 19/40 Iteration: 4700 | Train loss: 0.04803\n",
      "Epoch: 19/40 Iteration: 4720 | Train loss: 0.00531\n",
      "Epoch: 19/40 Iteration: 4740 | Train loss: 0.00357\n",
      "Epoch: 20/40 Iteration: 4760 | Train loss: 0.00078\n",
      "Epoch: 20/40 Iteration: 4780 | Train loss: 0.00090\n",
      "Epoch: 20/40 Iteration: 4800 | Train loss: 0.03966\n",
      "Epoch: 20/40 Iteration: 4820 | Train loss: 0.00054\n",
      "Epoch: 20/40 Iteration: 4840 | Train loss: 0.00192\n",
      "Epoch: 20/40 Iteration: 4860 | Train loss: 0.00051\n",
      "Epoch: 20/40 Iteration: 4880 | Train loss: 0.00073\n",
      "Epoch: 20/40 Iteration: 4900 | Train loss: 0.00031\n",
      "Epoch: 20/40 Iteration: 4920 | Train loss: 0.00201\n",
      "Epoch: 20/40 Iteration: 4940 | Train loss: 0.00369\n",
      "Epoch: 20/40 Iteration: 4960 | Train loss: 0.00037\n",
      "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00130\n",
      "Epoch: 20/40 Iteration: 5000 | Train loss: 0.00086\n",
      "Epoch: 21/40 Iteration: 5020 | Train loss: 0.00069\n",
      "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00210\n",
      "Epoch: 21/40 Iteration: 5060 | Train loss: 0.00170\n",
      "Epoch: 21/40 Iteration: 5080 | Train loss: 0.00090\n",
      "Epoch: 21/40 Iteration: 5100 | Train loss: 0.01094\n",
      "Epoch: 21/40 Iteration: 5120 | Train loss: 0.01833\n",
      "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00047\n",
      "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00447\n",
      "Epoch: 21/40 Iteration: 5180 | Train loss: 0.07116\n",
      "Epoch: 21/40 Iteration: 5200 | Train loss: 0.01179\n",
      "Epoch: 21/40 Iteration: 5220 | Train loss: 0.04242\n",
      "Epoch: 21/40 Iteration: 5240 | Train loss: 0.00323\n",
      "Epoch: 22/40 Iteration: 5260 | Train loss: 0.00087\n",
      "Epoch: 22/40 Iteration: 5280 | Train loss: 0.00118\n",
      "Epoch: 22/40 Iteration: 5300 | Train loss: 0.01016\n",
      "Epoch: 22/40 Iteration: 5320 | Train loss: 0.00139\n",
      "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00245\n",
      "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00178\n",
      "Epoch: 22/40 Iteration: 5380 | Train loss: 0.03093\n",
      "Epoch: 22/40 Iteration: 5400 | Train loss: 0.00022\n",
      "Epoch: 22/40 Iteration: 5420 | Train loss: 0.00234\n",
      "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00321\n",
      "Epoch: 22/40 Iteration: 5460 | Train loss: 0.00120\n",
      "Epoch: 22/40 Iteration: 5480 | Train loss: 0.00270\n",
      "Epoch: 22/40 Iteration: 5500 | Train loss: 0.00161\n",
      "Epoch: 23/40 Iteration: 5520 | Train loss: 0.02687\n",
      "Epoch: 23/40 Iteration: 5540 | Train loss: 0.01183\n",
      "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00267\n",
      "Epoch: 23/40 Iteration: 5580 | Train loss: 0.00132\n",
      "Epoch: 23/40 Iteration: 5600 | Train loss: 0.00782\n",
      "Epoch: 23/40 Iteration: 5620 | Train loss: 0.00738\n",
      "Epoch: 23/40 Iteration: 5640 | Train loss: 0.00042\n",
      "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00411\n",
      "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00028\n",
      "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00120\n",
      "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00311\n",
      "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00480\n",
      "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00751\n",
      "Epoch: 24/40 Iteration: 5780 | Train loss: 0.01778\n",
      "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00114\n",
      "Epoch: 24/40 Iteration: 5820 | Train loss: 0.00430\n",
      "Epoch: 24/40 Iteration: 5840 | Train loss: 0.03910\n",
      "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00277\n",
      "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00565\n",
      "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00186\n",
      "Epoch: 24/40 Iteration: 5920 | Train loss: 0.00104\n",
      "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00168\n",
      "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00313\n",
      "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00039\n",
      "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00368\n",
      "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00666\n",
      "Epoch: 25/40 Iteration: 6040 | Train loss: 0.00040\n",
      "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00653\n",
      "Epoch: 25/40 Iteration: 6080 | Train loss: 0.00133\n",
      "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00063\n",
      "Epoch: 25/40 Iteration: 6120 | Train loss: 0.02298\n",
      "Epoch: 25/40 Iteration: 6140 | Train loss: 0.00710\n",
      "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00075\n",
      "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00045\n",
      "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00119\n",
      "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00027\n",
      "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00587\n",
      "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00640\n",
      "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00312\n",
      "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00045\n",
      "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00053\n",
      "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00385\n",
      "Epoch: 26/40 Iteration: 6360 | Train loss: 0.00014\n",
      "Epoch: 26/40 Iteration: 6380 | Train loss: 0.00084\n",
      "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00040\n",
      "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00014\n",
      "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00041\n",
      "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00016\n",
      "Epoch: 26/40 Iteration: 6480 | Train loss: 0.04348\n",
      "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40 Iteration: 6520 | Train loss: 0.01708\n",
      "Epoch: 27/40 Iteration: 6540 | Train loss: 0.00191\n",
      "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00075\n",
      "Epoch: 27/40 Iteration: 6580 | Train loss: 0.01188\n",
      "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00109\n",
      "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00296\n",
      "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00023\n",
      "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00484\n",
      "Epoch: 27/40 Iteration: 6680 | Train loss: 0.01334\n",
      "Epoch: 27/40 Iteration: 6700 | Train loss: 0.00139\n",
      "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00065\n",
      "Epoch: 27/40 Iteration: 6740 | Train loss: 0.00211\n",
      "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00094\n",
      "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00521\n",
      "Epoch: 28/40 Iteration: 6800 | Train loss: 0.00300\n",
      "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00055\n",
      "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00024\n",
      "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00032\n",
      "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00079\n",
      "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00191\n",
      "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00015\n",
      "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00064\n",
      "Epoch: 28/40 Iteration: 6960 | Train loss: 0.00041\n",
      "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00052\n",
      "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00310\n",
      "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00018\n",
      "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00012\n",
      "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00220\n",
      "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00771\n",
      "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00046\n",
      "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00075\n",
      "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00017\n",
      "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00022\n",
      "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00012\n",
      "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00018\n",
      "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00017\n",
      "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00042\n",
      "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00012\n",
      "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00023\n",
      "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00040\n",
      "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00111\n",
      "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00107\n",
      "Epoch: 30/40 Iteration: 7380 | Train loss: 0.02679\n",
      "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00259\n",
      "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00609\n",
      "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00209\n",
      "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00018\n",
      "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00024\n",
      "Epoch: 30/40 Iteration: 7500 | Train loss: 0.03528\n",
      "Epoch: 31/40 Iteration: 7520 | Train loss: 0.03855\n",
      "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00483\n",
      "Epoch: 31/40 Iteration: 7560 | Train loss: 0.02236\n",
      "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00407\n",
      "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00381\n",
      "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00081\n",
      "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00019\n",
      "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00012\n",
      "Epoch: 31/40 Iteration: 7680 | Train loss: 0.01369\n",
      "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00159\n",
      "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00038\n",
      "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00380\n",
      "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00060\n",
      "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00060\n",
      "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00199\n",
      "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00021\n",
      "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00014\n",
      "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00005\n",
      "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00025\n",
      "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00634\n",
      "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00030\n",
      "Epoch: 32/40 Iteration: 7940 | Train loss: 0.04993\n",
      "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00031\n",
      "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00007\n",
      "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00078\n",
      "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00096\n",
      "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00030\n",
      "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00070\n",
      "Epoch: 33/40 Iteration: 8080 | Train loss: 0.03299\n",
      "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00341\n",
      "Epoch: 33/40 Iteration: 8120 | Train loss: 0.01103\n",
      "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00006\n",
      "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00030\n",
      "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00036\n",
      "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00018\n",
      "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00244\n",
      "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00017\n",
      "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00037\n",
      "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00058\n",
      "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00005\n",
      "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00014\n",
      "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00119\n",
      "Epoch: 34/40 Iteration: 8360 | Train loss: 0.02289\n",
      "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00133\n",
      "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00042\n",
      "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00033\n",
      "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00156\n",
      "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00007\n",
      "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00038\n",
      "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00101\n",
      "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00009\n",
      "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00031\n",
      "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00062\n",
      "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00011\n",
      "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00015\n",
      "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00599\n",
      "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00015\n",
      "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00007\n",
      "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00004\n",
      "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00004\n",
      "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00023\n",
      "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00008\n",
      "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00033\n",
      "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00016\n",
      "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00044\n",
      "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00007\n",
      "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00029\n",
      "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00011\n",
      "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00036\n",
      "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00011\n",
      "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00011\n",
      "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00006\n",
      "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00010\n",
      "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00016\n",
      "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00014\n",
      "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00005\n",
      "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00007\n",
      "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00011\n",
      "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00020\n",
      "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00020\n",
      "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00014\n",
      "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00005\n",
      "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00003\n",
      "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00003\n",
      "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00012\n",
      "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00004\n",
      "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00014\n",
      "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00006\n",
      "Epoch: 38/40 Iteration: 9300 | Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00006\n",
      "Epoch: 38/40 Iteration: 9380 | Train loss: 0.00010\n",
      "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00011\n",
      "Epoch: 38/40 Iteration: 9420 | Train loss: 0.00013\n",
      "Epoch: 38/40 Iteration: 9440 | Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9460 | Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9500 | Train loss: 0.00003\n",
      "Epoch: 39/40 Iteration: 9520 | Train loss: 0.00003\n",
      "Epoch: 39/40 Iteration: 9540 | Train loss: 0.00004\n",
      "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00002\n",
      "Epoch: 39/40 Iteration: 9580 | Train loss: 0.00003\n",
      "Epoch: 39/40 Iteration: 9600 | Train loss: 0.00004\n",
      "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00007\n",
      "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00004\n",
      "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00015\n",
      "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00002\n",
      "Epoch: 40/40 Iteration: 9880 | Train loss: 0.00008\n",
      "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00003\n",
      "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00005\n",
      "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00003\n",
      "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00003\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc.: 0.860\n"
     ]
    }
   ],
   "source": [
    "## Test: \n",
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (\n",
    "      np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get probabilities:\n",
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 16.3.2. Example application: character-level language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 16.3.2.1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Reading and processing text\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch:i for i,ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text],\n",
    "                     dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @Readers: PLEASE IGNORE THIS CELL\n",
    "##\n",
    "## This cell is meant to shrink the\n",
    "## dataset when this notebook is run\n",
    "## on the Travis Continuous Integration\n",
    "## platform to test the code as well as\n",
    "## speeding up the run using a smaller\n",
    "## dataset for debugging\n",
    "\n",
    "if 'TRAVIS' in os.environ:\n",
    "    text = text[:1000]\n",
    "    chars = set(text)\n",
    "    char2int = {ch:i for i,ch in enumerate(chars)}\n",
    "    int2char = dict(enumerate(chars))\n",
    "    text_ints = np.array([char2int[ch] for ch in text],\n",
    "                         dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2540)\n",
      "[ 4 31 60 15  4 16 23 28 60 19]\n",
      "[31 60 15  4 16 23 28 60 19 57]\n",
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima\n"
     ]
    }
   ],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    mini_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / mini_batch_length)\n",
    "    if num_batches*mini_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    ## Truncate the sequence at the end to get rid of \n",
    "    ## remaining charcaters that do not make a full batch\n",
    "    x = sequence[0 : num_batches*mini_batch_length]\n",
    "    y = sequence[1 : num_batches*mini_batch_length + 1]\n",
    "    ## Split x & y into a list batches of sequences:\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    ## Stack the batches together\n",
    "    ## batch_size x mini_batch_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "## Testing:\n",
    "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
    "print(train_x.shape)\n",
    "print(train_x[0, :10])\n",
    "print(train_y[0, :10])\n",
    "print(''.join(int2char[i] for i in train_x[0, :50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15) (64, 15)  The Tragedie of      he Tragedie of \n",
      "(64, 15) (64, 15)   Hamlet**Actus       Hamlet**Actus P\n",
      "(64, 15) (64, 15)  Primus. Scoena       rimus. Scoena P\n",
      "(64, 15) (64, 15)  Prima.**Enter B      rima.**Enter Ba\n",
      "(64, 15) (64, 15)  arnardo and Fra      rnardo and Fran\n",
      "(64, 15) (64, 15)  ncisco two Cent      cisco two Centi\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps: (b+1)*num_steps],\n",
    "               data_y[:, b*num_steps: (b+1)*num_steps])\n",
    "\n",
    "bgen = create_batch_generator(train_x[:,:100], train_y[:,:100], 15)\n",
    "for b in bgen:\n",
    "    print(b[0].shape, b[1].shape, end='  ')\n",
    "    print(''.join(int2char[i] for i in b[0][0,:]).replace('\\n', '*'), '    ',\n",
    "          ''.join(int2char[i] for i in b[1][0,:]).replace('\\n', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 16.3.2.2. Building the character-level RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64,\n",
    "                 num_steps=100, lstm_size=128,\n",
    "                 num_layers=1, learning_rate=0.001,\n",
    "                 keep_prob=0.5, grad_clip=5,\n",
    "                 sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                              shape=[batch_size, num_steps],\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32,\n",
    "                              shape=[batch_size, num_steps],\n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                              name='tf_keepprob')\n",
    "\n",
    "        # One-hot encoding:\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        ### Build the multi-layer RNN cells\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
    "                output_keep_prob=tf_keepprob)\n",
    "            for _ in range(self.num_layers)])\n",
    "\n",
    "        ## Define the initial state\n",
    "        self.initial_state = cells.zero_state(\n",
    "                    batch_size, tf.float32)\n",
    "\n",
    "        ## Run each sequence step through the RNN\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                    cells, x_onehot,\n",
    "                    initial_state=self.initial_state)\n",
    "\n",
    "        print('  << lstm_outputs  >>', lstm_outputs)\n",
    "\n",
    "        seq_output_reshaped = tf.reshape(\n",
    "                    lstm_outputs,\n",
    "                    shape=[-1, self.lstm_size],\n",
    "                    name='seq_output_reshaped')\n",
    "\n",
    "        logits = tf.layers.dense(\n",
    "                    inputs=seq_output_reshaped,\n",
    "                    units=self.num_classes,\n",
    "                    activation=None,\n",
    "                    name='logits')\n",
    "\n",
    "        proba = tf.nn.softmax(\n",
    "                    logits,\n",
    "                    name='probabilities')\n",
    "        print(proba)\n",
    "\n",
    "        y_reshaped = tf.reshape(\n",
    "                    y_onehot,\n",
    "                    shape=[-1, self.num_classes],\n",
    "                    name='y_reshaped')\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits,\n",
    "                        labels=y_reshaped),\n",
    "                    name='cost')\n",
    "\n",
    "        # Gradient clipping to avoid \"exploding gradients\"\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                    tf.gradients(cost, tvars),\n",
    "                    self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    name='train_op')\n",
    "\n",
    "    def train(self, train_x, train_y,\n",
    "              num_epochs, ckpt_dir='./model/'):\n",
    "        ## Create the checkpoint directory\n",
    "        ## if does not exists\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # Train network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                ## Minibatch generator:\n",
    "                bgen = create_batch_generator(\n",
    "                        train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': self.keep_prob,\n",
    "                            self.initial_state : new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                                self.final_state],\n",
    "                            feed_dict=feed)\n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: %.4f' % (\n",
    "                              epoch + 1, num_epochs, \n",
    "                              iteration, batch_cost))\n",
    "\n",
    "                ## Save the trained model\n",
    "                self.saver.save(\n",
    "                        sess, os.path.join(\n",
    "                            ckpt_dir, 'language_modeling.ckpt'))\n",
    "\n",
    "    def sample(self, output_length,\n",
    "               ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess,\n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "            ## 1: run the model using the starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "\n",
    "            ## 2: run the model using the updated observed_seq\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(6400, 65), dtype=float32)\n",
      "Epoch 1/100 Iteration 10| Training loss: 3.6380\n",
      "Epoch 1/100 Iteration 20| Training loss: 3.3799\n",
      "Epoch 2/100 Iteration 30| Training loss: 3.3015\n",
      "Epoch 2/100 Iteration 40| Training loss: 3.2404\n",
      "Epoch 2/100 Iteration 50| Training loss: 3.2236\n",
      "Epoch 3/100 Iteration 60| Training loss: 3.2259\n",
      "Epoch 3/100 Iteration 70| Training loss: 3.1966\n",
      "Epoch 4/100 Iteration 80| Training loss: 3.1717\n",
      "Epoch 4/100 Iteration 90| Training loss: 3.1432\n",
      "Epoch 4/100 Iteration 100| Training loss: 3.1185\n",
      "Epoch 5/100 Iteration 110| Training loss: 3.1117\n",
      "Epoch 5/100 Iteration 120| Training loss: 3.0710\n",
      "Epoch 6/100 Iteration 130| Training loss: 3.0249\n",
      "Epoch 6/100 Iteration 140| Training loss: 2.9932\n",
      "Epoch 6/100 Iteration 150| Training loss: 2.9558\n",
      "Epoch 7/100 Iteration 160| Training loss: 2.9258\n",
      "Epoch 7/100 Iteration 170| Training loss: 2.8774\n",
      "Epoch 8/100 Iteration 180| Training loss: 2.8378\n",
      "Epoch 8/100 Iteration 190| Training loss: 2.8006\n",
      "Epoch 8/100 Iteration 200| Training loss: 2.7596\n",
      "Epoch 9/100 Iteration 210| Training loss: 2.7404\n",
      "Epoch 9/100 Iteration 220| Training loss: 2.6808\n",
      "Epoch 10/100 Iteration 230| Training loss: 2.6526\n",
      "Epoch 10/100 Iteration 240| Training loss: 2.6452\n",
      "Epoch 10/100 Iteration 250| Training loss: 2.5905\n",
      "Epoch 11/100 Iteration 260| Training loss: 2.5955\n",
      "Epoch 11/100 Iteration 270| Training loss: 2.5462\n",
      "Epoch 12/100 Iteration 280| Training loss: 2.5245\n",
      "Epoch 12/100 Iteration 290| Training loss: 2.5343\n",
      "Epoch 12/100 Iteration 300| Training loss: 2.4687\n",
      "Epoch 13/100 Iteration 310| Training loss: 2.4943\n",
      "Epoch 13/100 Iteration 320| Training loss: 2.4511\n",
      "Epoch 14/100 Iteration 330| Training loss: 2.4399\n",
      "Epoch 14/100 Iteration 340| Training loss: 2.4547\n",
      "Epoch 14/100 Iteration 350| Training loss: 2.4029\n",
      "Epoch 15/100 Iteration 360| Training loss: 2.4357\n",
      "Epoch 15/100 Iteration 370| Training loss: 2.3890\n",
      "Epoch 16/100 Iteration 380| Training loss: 2.3822\n",
      "Epoch 16/100 Iteration 390| Training loss: 2.3983\n",
      "Epoch 16/100 Iteration 400| Training loss: 2.3439\n",
      "Epoch 17/100 Iteration 410| Training loss: 2.3760\n",
      "Epoch 17/100 Iteration 420| Training loss: 2.3433\n",
      "Epoch 18/100 Iteration 430| Training loss: 2.3444\n",
      "Epoch 18/100 Iteration 440| Training loss: 2.3513\n",
      "Epoch 18/100 Iteration 450| Training loss: 2.2964\n",
      "Epoch 19/100 Iteration 460| Training loss: 2.3528\n",
      "Epoch 19/100 Iteration 470| Training loss: 2.3051\n",
      "Epoch 20/100 Iteration 480| Training loss: 2.3042\n",
      "Epoch 20/100 Iteration 490| Training loss: 2.3276\n",
      "Epoch 20/100 Iteration 500| Training loss: 2.2731\n",
      "Epoch 21/100 Iteration 510| Training loss: 2.3223\n",
      "Epoch 21/100 Iteration 520| Training loss: 2.2692\n",
      "Epoch 22/100 Iteration 530| Training loss: 2.2809\n",
      "Epoch 22/100 Iteration 540| Training loss: 2.3018\n",
      "Epoch 22/100 Iteration 550| Training loss: 2.2503\n",
      "Epoch 23/100 Iteration 560| Training loss: 2.2990\n",
      "Epoch 23/100 Iteration 570| Training loss: 2.2626\n",
      "Epoch 24/100 Iteration 580| Training loss: 2.2603\n",
      "Epoch 24/100 Iteration 590| Training loss: 2.2762\n",
      "Epoch 24/100 Iteration 600| Training loss: 2.2113\n",
      "Epoch 25/100 Iteration 610| Training loss: 2.2614\n",
      "Epoch 25/100 Iteration 620| Training loss: 2.2322\n",
      "Epoch 26/100 Iteration 630| Training loss: 2.2313\n",
      "Epoch 26/100 Iteration 640| Training loss: 2.2620\n",
      "Epoch 26/100 Iteration 650| Training loss: 2.1923\n",
      "Epoch 27/100 Iteration 660| Training loss: 2.2463\n",
      "Epoch 27/100 Iteration 670| Training loss: 2.2108\n",
      "Epoch 28/100 Iteration 680| Training loss: 2.2192\n",
      "Epoch 28/100 Iteration 690| Training loss: 2.2351\n",
      "Epoch 28/100 Iteration 700| Training loss: 2.1825\n",
      "Epoch 29/100 Iteration 710| Training loss: 2.2195\n",
      "Epoch 29/100 Iteration 720| Training loss: 2.1998\n",
      "Epoch 30/100 Iteration 730| Training loss: 2.1970\n",
      "Epoch 30/100 Iteration 740| Training loss: 2.2237\n",
      "Epoch 30/100 Iteration 750| Training loss: 2.1610\n",
      "Epoch 31/100 Iteration 760| Training loss: 2.2064\n",
      "Epoch 31/100 Iteration 770| Training loss: 2.1730\n",
      "Epoch 32/100 Iteration 780| Training loss: 2.1732\n",
      "Epoch 32/100 Iteration 790| Training loss: 2.1983\n",
      "Epoch 32/100 Iteration 800| Training loss: 2.1453\n",
      "Epoch 33/100 Iteration 810| Training loss: 2.1896\n",
      "Epoch 33/100 Iteration 820| Training loss: 2.1657\n",
      "Epoch 34/100 Iteration 830| Training loss: 2.1628\n",
      "Epoch 34/100 Iteration 840| Training loss: 2.1919\n",
      "Epoch 34/100 Iteration 850| Training loss: 2.1118\n",
      "Epoch 35/100 Iteration 860| Training loss: 2.1807\n",
      "Epoch 35/100 Iteration 870| Training loss: 2.1465\n",
      "Epoch 36/100 Iteration 880| Training loss: 2.1444\n",
      "Epoch 36/100 Iteration 890| Training loss: 2.1786\n",
      "Epoch 36/100 Iteration 900| Training loss: 2.1020\n",
      "Epoch 37/100 Iteration 910| Training loss: 2.1642\n",
      "Epoch 37/100 Iteration 920| Training loss: 2.1355\n",
      "Epoch 38/100 Iteration 930| Training loss: 2.1340\n",
      "Epoch 38/100 Iteration 940| Training loss: 2.1572\n",
      "Epoch 38/100 Iteration 950| Training loss: 2.0904\n",
      "Epoch 39/100 Iteration 960| Training loss: 2.1505\n",
      "Epoch 39/100 Iteration 970| Training loss: 2.1168\n",
      "Epoch 40/100 Iteration 980| Training loss: 2.1245\n",
      "Epoch 40/100 Iteration 990| Training loss: 2.1599\n",
      "Epoch 40/100 Iteration 1000| Training loss: 2.0851\n",
      "Epoch 41/100 Iteration 1010| Training loss: 2.1236\n",
      "Epoch 41/100 Iteration 1020| Training loss: 2.1108\n",
      "Epoch 42/100 Iteration 1030| Training loss: 2.1081\n",
      "Epoch 42/100 Iteration 1040| Training loss: 2.1397\n",
      "Epoch 42/100 Iteration 1050| Training loss: 2.0700\n",
      "Epoch 43/100 Iteration 1060| Training loss: 2.1171\n",
      "Epoch 43/100 Iteration 1070| Training loss: 2.1021\n",
      "Epoch 44/100 Iteration 1080| Training loss: 2.1000\n",
      "Epoch 44/100 Iteration 1090| Training loss: 2.1129\n",
      "Epoch 44/100 Iteration 1100| Training loss: 2.0555\n",
      "Epoch 45/100 Iteration 1110| Training loss: 2.1075\n",
      "Epoch 45/100 Iteration 1120| Training loss: 2.0680\n",
      "Epoch 46/100 Iteration 1130| Training loss: 2.0830\n",
      "Epoch 46/100 Iteration 1140| Training loss: 2.1140\n",
      "Epoch 46/100 Iteration 1150| Training loss: 2.0391\n",
      "Epoch 47/100 Iteration 1160| Training loss: 2.0984\n",
      "Epoch 47/100 Iteration 1170| Training loss: 2.0599\n",
      "Epoch 48/100 Iteration 1180| Training loss: 2.0650\n",
      "Epoch 48/100 Iteration 1190| Training loss: 2.1063\n",
      "Epoch 48/100 Iteration 1200| Training loss: 2.0353\n",
      "Epoch 49/100 Iteration 1210| Training loss: 2.0865\n",
      "Epoch 49/100 Iteration 1220| Training loss: 2.0533\n",
      "Epoch 50/100 Iteration 1230| Training loss: 2.0721\n",
      "Epoch 50/100 Iteration 1240| Training loss: 2.0931\n",
      "Epoch 50/100 Iteration 1250| Training loss: 2.0268\n",
      "Epoch 51/100 Iteration 1260| Training loss: 2.0749\n",
      "Epoch 51/100 Iteration 1270| Training loss: 2.0439\n",
      "Epoch 52/100 Iteration 1280| Training loss: 2.0489\n",
      "Epoch 52/100 Iteration 1290| Training loss: 2.0772\n",
      "Epoch 52/100 Iteration 1300| Training loss: 2.0097\n",
      "Epoch 53/100 Iteration 1310| Training loss: 2.0522\n",
      "Epoch 53/100 Iteration 1320| Training loss: 2.0318\n",
      "Epoch 54/100 Iteration 1330| Training loss: 2.0373\n",
      "Epoch 54/100 Iteration 1340| Training loss: 2.0609\n",
      "Epoch 54/100 Iteration 1350| Training loss: 1.9988\n",
      "Epoch 55/100 Iteration 1360| Training loss: 2.0560\n",
      "Epoch 55/100 Iteration 1370| Training loss: 2.0271\n",
      "Epoch 56/100 Iteration 1380| Training loss: 2.0414\n",
      "Epoch 56/100 Iteration 1390| Training loss: 2.0595\n",
      "Epoch 56/100 Iteration 1400| Training loss: 1.9874\n",
      "Epoch 57/100 Iteration 1410| Training loss: 2.0441\n",
      "Epoch 57/100 Iteration 1420| Training loss: 2.0159\n",
      "Epoch 58/100 Iteration 1430| Training loss: 2.0093\n",
      "Epoch 58/100 Iteration 1440| Training loss: 2.0444\n",
      "Epoch 58/100 Iteration 1450| Training loss: 1.9808\n",
      "Epoch 59/100 Iteration 1460| Training loss: 2.0307\n",
      "Epoch 59/100 Iteration 1470| Training loss: 2.0073\n",
      "Epoch 60/100 Iteration 1480| Training loss: 2.0038\n",
      "Epoch 60/100 Iteration 1490| Training loss: 2.0411\n",
      "Epoch 60/100 Iteration 1500| Training loss: 1.9669\n",
      "Epoch 61/100 Iteration 1510| Training loss: 2.0058\n",
      "Epoch 61/100 Iteration 1520| Training loss: 1.9984\n",
      "Epoch 62/100 Iteration 1530| Training loss: 1.9975\n",
      "Epoch 62/100 Iteration 1540| Training loss: 2.0257\n",
      "Epoch 62/100 Iteration 1550| Training loss: 1.9617\n",
      "Epoch 63/100 Iteration 1560| Training loss: 2.0156\n",
      "Epoch 63/100 Iteration 1570| Training loss: 2.0023\n",
      "Epoch 64/100 Iteration 1580| Training loss: 1.9869\n",
      "Epoch 64/100 Iteration 1590| Training loss: 2.0107\n",
      "Epoch 64/100 Iteration 1600| Training loss: 1.9561\n",
      "Epoch 65/100 Iteration 1610| Training loss: 1.9938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 Iteration 1620| Training loss: 1.9822\n",
      "Epoch 66/100 Iteration 1630| Training loss: 1.9823\n",
      "Epoch 66/100 Iteration 1640| Training loss: 2.0193\n",
      "Epoch 66/100 Iteration 1650| Training loss: 1.9470\n",
      "Epoch 67/100 Iteration 1660| Training loss: 1.9888\n",
      "Epoch 67/100 Iteration 1670| Training loss: 1.9726\n",
      "Epoch 68/100 Iteration 1680| Training loss: 1.9785\n",
      "Epoch 68/100 Iteration 1690| Training loss: 2.0012\n",
      "Epoch 68/100 Iteration 1700| Training loss: 1.9358\n",
      "Epoch 69/100 Iteration 1710| Training loss: 1.9881\n",
      "Epoch 69/100 Iteration 1720| Training loss: 1.9632\n",
      "Epoch 70/100 Iteration 1730| Training loss: 1.9606\n",
      "Epoch 70/100 Iteration 1740| Training loss: 1.9986\n",
      "Epoch 70/100 Iteration 1750| Training loss: 1.9288\n",
      "Epoch 71/100 Iteration 1760| Training loss: 1.9731\n",
      "Epoch 71/100 Iteration 1770| Training loss: 1.9560\n",
      "Epoch 72/100 Iteration 1780| Training loss: 1.9580\n",
      "Epoch 72/100 Iteration 1790| Training loss: 1.9895\n",
      "Epoch 72/100 Iteration 1800| Training loss: 1.9249\n",
      "Epoch 73/100 Iteration 1810| Training loss: 1.9701\n",
      "Epoch 73/100 Iteration 1820| Training loss: 1.9397\n",
      "Epoch 74/100 Iteration 1830| Training loss: 1.9560\n",
      "Epoch 74/100 Iteration 1840| Training loss: 1.9737\n",
      "Epoch 74/100 Iteration 1850| Training loss: 1.9287\n",
      "Epoch 75/100 Iteration 1860| Training loss: 1.9576\n",
      "Epoch 75/100 Iteration 1870| Training loss: 1.9400\n",
      "Epoch 76/100 Iteration 1880| Training loss: 1.9383\n",
      "Epoch 76/100 Iteration 1890| Training loss: 1.9691\n",
      "Epoch 76/100 Iteration 1900| Training loss: 1.8990\n",
      "Epoch 77/100 Iteration 1910| Training loss: 1.9523\n",
      "Epoch 77/100 Iteration 1920| Training loss: 1.9432\n",
      "Epoch 78/100 Iteration 1930| Training loss: 1.9433\n",
      "Epoch 78/100 Iteration 1940| Training loss: 1.9699\n",
      "Epoch 78/100 Iteration 1950| Training loss: 1.9021\n",
      "Epoch 79/100 Iteration 1960| Training loss: 1.9419\n",
      "Epoch 79/100 Iteration 1970| Training loss: 1.9179\n",
      "Epoch 80/100 Iteration 1980| Training loss: 1.9292\n",
      "Epoch 80/100 Iteration 1990| Training loss: 1.9552\n",
      "Epoch 80/100 Iteration 2000| Training loss: 1.8843\n",
      "Epoch 81/100 Iteration 2010| Training loss: 1.9336\n",
      "Epoch 81/100 Iteration 2020| Training loss: 1.9245\n",
      "Epoch 82/100 Iteration 2030| Training loss: 1.9177\n",
      "Epoch 82/100 Iteration 2040| Training loss: 1.9599\n",
      "Epoch 82/100 Iteration 2050| Training loss: 1.8981\n",
      "Epoch 83/100 Iteration 2060| Training loss: 1.9358\n",
      "Epoch 83/100 Iteration 2070| Training loss: 1.9190\n",
      "Epoch 84/100 Iteration 2080| Training loss: 1.9193\n",
      "Epoch 84/100 Iteration 2090| Training loss: 1.9548\n",
      "Epoch 84/100 Iteration 2100| Training loss: 1.8902\n",
      "Epoch 85/100 Iteration 2110| Training loss: 1.9284\n",
      "Epoch 85/100 Iteration 2120| Training loss: 1.9187\n",
      "Epoch 86/100 Iteration 2130| Training loss: 1.9150\n",
      "Epoch 86/100 Iteration 2140| Training loss: 1.9325\n",
      "Epoch 86/100 Iteration 2150| Training loss: 1.8740\n",
      "Epoch 87/100 Iteration 2160| Training loss: 1.9138\n",
      "Epoch 87/100 Iteration 2170| Training loss: 1.9110\n",
      "Epoch 88/100 Iteration 2180| Training loss: 1.9098\n",
      "Epoch 88/100 Iteration 2190| Training loss: 1.9408\n",
      "Epoch 88/100 Iteration 2200| Training loss: 1.8684\n",
      "Epoch 89/100 Iteration 2210| Training loss: 1.9136\n",
      "Epoch 89/100 Iteration 2220| Training loss: 1.8907\n",
      "Epoch 90/100 Iteration 2230| Training loss: 1.8989\n",
      "Epoch 90/100 Iteration 2240| Training loss: 1.9290\n",
      "Epoch 90/100 Iteration 2250| Training loss: 1.8751\n",
      "Epoch 91/100 Iteration 2260| Training loss: 1.9101\n",
      "Epoch 91/100 Iteration 2270| Training loss: 1.8917\n",
      "Epoch 92/100 Iteration 2280| Training loss: 1.9003\n",
      "Epoch 92/100 Iteration 2290| Training loss: 1.9266\n",
      "Epoch 92/100 Iteration 2300| Training loss: 1.8630\n",
      "Epoch 93/100 Iteration 2310| Training loss: 1.8908\n",
      "Epoch 93/100 Iteration 2320| Training loss: 1.8851\n",
      "Epoch 94/100 Iteration 2330| Training loss: 1.8843\n",
      "Epoch 94/100 Iteration 2340| Training loss: 1.9137\n",
      "Epoch 94/100 Iteration 2350| Training loss: 1.8500\n",
      "Epoch 95/100 Iteration 2360| Training loss: 1.8948\n",
      "Epoch 95/100 Iteration 2370| Training loss: 1.8752\n",
      "Epoch 96/100 Iteration 2380| Training loss: 1.8912\n",
      "Epoch 96/100 Iteration 2390| Training loss: 1.9180\n",
      "Epoch 96/100 Iteration 2400| Training loss: 1.8497\n",
      "Epoch 97/100 Iteration 2410| Training loss: 1.8761\n",
      "Epoch 97/100 Iteration 2420| Training loss: 1.8768\n",
      "Epoch 98/100 Iteration 2430| Training loss: 1.8744\n",
      "Epoch 98/100 Iteration 2440| Training loss: 1.9094\n",
      "Epoch 98/100 Iteration 2450| Training loss: 1.8491\n",
      "Epoch 99/100 Iteration 2460| Training loss: 1.8863\n",
      "Epoch 99/100 Iteration 2470| Training loss: 1.8604\n",
      "Epoch 100/100 Iteration 2480| Training loss: 1.8740\n",
      "Epoch 100/100 Iteration 2490| Training loss: 1.9109\n",
      "Epoch 100/100 Iteration 2500| Training loss: 1.8539\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints,\n",
    "                                batch_size,\n",
    "                                num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y,\n",
    "          num_epochs=100,\n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(1, 65), dtype=float32)\n",
      "The martient,\n",
      "In a marther, what wall mare\n",
      "\n",
      "   Hor. No this mint oruestall thes worth\n",
      "\n",
      "   Ham. How way all a mere ard berise, and, with the mat, and that\n",
      "That sine mersingethis, if all a thes it thes and wate\n",
      "\n",
      "   King. I dis noth mars all\n",
      "The my that\n",
      " he meare ta mye dast masst sill to tore\n",
      "Weathe in arress it to mardere of im. In ither of how,\n",
      "What then as as our a thimend thou to the bott all make it is, thes on\n",
      "\n",
      "   Hor. Whene my Lord, whore and wasthandssing the marther\n",
      "As and thes whe ting inthe \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "\n",
    "print(rnn.sample(ckpt_dir='./model-100/',\n",
    "                 output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(6400, 65), dtype=float32)\n",
      "Epoch 1/200 Iteration 10| Training loss: 3.6378\n",
      "Epoch 1/200 Iteration 20| Training loss: 3.3800\n",
      "Epoch 2/200 Iteration 30| Training loss: 3.3015\n",
      "Epoch 2/200 Iteration 40| Training loss: 3.2459\n",
      "Epoch 2/200 Iteration 50| Training loss: 3.2221\n",
      "Epoch 3/200 Iteration 60| Training loss: 3.2261\n",
      "Epoch 3/200 Iteration 70| Training loss: 3.1969\n",
      "Epoch 4/200 Iteration 80| Training loss: 3.1734\n",
      "Epoch 4/200 Iteration 90| Training loss: 3.1437\n",
      "Epoch 4/200 Iteration 100| Training loss: 3.1177\n",
      "Epoch 5/200 Iteration 110| Training loss: 3.1133\n",
      "Epoch 5/200 Iteration 120| Training loss: 3.0729\n",
      "Epoch 6/200 Iteration 130| Training loss: 3.0300\n",
      "Epoch 6/200 Iteration 140| Training loss: 2.9962\n",
      "Epoch 6/200 Iteration 150| Training loss: 2.9531\n",
      "Epoch 7/200 Iteration 160| Training loss: 2.9273\n",
      "Epoch 7/200 Iteration 170| Training loss: 2.8783\n",
      "Epoch 8/200 Iteration 180| Training loss: 2.8357\n",
      "Epoch 8/200 Iteration 190| Training loss: 2.8029\n",
      "Epoch 8/200 Iteration 200| Training loss: 2.7541\n",
      "Epoch 9/200 Iteration 210| Training loss: 2.7412\n",
      "Epoch 9/200 Iteration 220| Training loss: 2.6823\n",
      "Epoch 10/200 Iteration 230| Training loss: 2.6509\n",
      "Epoch 10/200 Iteration 240| Training loss: 2.6400\n",
      "Epoch 10/200 Iteration 250| Training loss: 2.5861\n",
      "Epoch 11/200 Iteration 260| Training loss: 2.5973\n",
      "Epoch 11/200 Iteration 270| Training loss: 2.5483\n",
      "Epoch 12/200 Iteration 280| Training loss: 2.5231\n",
      "Epoch 12/200 Iteration 290| Training loss: 2.5297\n",
      "Epoch 12/200 Iteration 300| Training loss: 2.4664\n",
      "Epoch 13/200 Iteration 310| Training loss: 2.4959\n",
      "Epoch 13/200 Iteration 320| Training loss: 2.4504\n",
      "Epoch 14/200 Iteration 330| Training loss: 2.4377\n",
      "Epoch 14/200 Iteration 340| Training loss: 2.4605\n",
      "Epoch 14/200 Iteration 350| Training loss: 2.4055\n",
      "Epoch 15/200 Iteration 360| Training loss: 2.4378\n",
      "Epoch 15/200 Iteration 370| Training loss: 2.3886\n",
      "Epoch 16/200 Iteration 380| Training loss: 2.3885\n",
      "Epoch 16/200 Iteration 390| Training loss: 2.3944\n",
      "Epoch 16/200 Iteration 400| Training loss: 2.3465\n",
      "Epoch 17/200 Iteration 410| Training loss: 2.3758\n",
      "Epoch 17/200 Iteration 420| Training loss: 2.3443\n",
      "Epoch 18/200 Iteration 430| Training loss: 2.3439\n",
      "Epoch 18/200 Iteration 440| Training loss: 2.3507\n",
      "Epoch 18/200 Iteration 450| Training loss: 2.2946\n",
      "Epoch 19/200 Iteration 460| Training loss: 2.3452\n",
      "Epoch 19/200 Iteration 470| Training loss: 2.3039\n",
      "Epoch 20/200 Iteration 480| Training loss: 2.3008\n",
      "Epoch 20/200 Iteration 490| Training loss: 2.3309\n",
      "Epoch 20/200 Iteration 500| Training loss: 2.2671\n",
      "Epoch 21/200 Iteration 510| Training loss: 2.3210\n",
      "Epoch 21/200 Iteration 520| Training loss: 2.2701\n",
      "Epoch 22/200 Iteration 530| Training loss: 2.2817\n",
      "Epoch 22/200 Iteration 540| Training loss: 2.3056\n",
      "Epoch 22/200 Iteration 550| Training loss: 2.2477\n",
      "Epoch 23/200 Iteration 560| Training loss: 2.2909\n",
      "Epoch 23/200 Iteration 570| Training loss: 2.2612\n",
      "Epoch 24/200 Iteration 580| Training loss: 2.2519\n",
      "Epoch 24/200 Iteration 590| Training loss: 2.2752\n",
      "Epoch 24/200 Iteration 600| Training loss: 2.2117\n",
      "Epoch 25/200 Iteration 610| Training loss: 2.2542\n",
      "Epoch 25/200 Iteration 620| Training loss: 2.2324\n",
      "Epoch 26/200 Iteration 630| Training loss: 2.2376\n",
      "Epoch 26/200 Iteration 640| Training loss: 2.2637\n",
      "Epoch 26/200 Iteration 650| Training loss: 2.1962\n",
      "Epoch 27/200 Iteration 660| Training loss: 2.2497\n",
      "Epoch 27/200 Iteration 670| Training loss: 2.2162\n",
      "Epoch 28/200 Iteration 680| Training loss: 2.2163\n",
      "Epoch 28/200 Iteration 690| Training loss: 2.2374\n",
      "Epoch 28/200 Iteration 700| Training loss: 2.1839\n",
      "Epoch 29/200 Iteration 710| Training loss: 2.2219\n",
      "Epoch 29/200 Iteration 720| Training loss: 2.2052\n",
      "Epoch 30/200 Iteration 730| Training loss: 2.1990\n",
      "Epoch 30/200 Iteration 740| Training loss: 2.2175\n",
      "Epoch 30/200 Iteration 750| Training loss: 2.1621\n",
      "Epoch 31/200 Iteration 760| Training loss: 2.2103\n",
      "Epoch 31/200 Iteration 770| Training loss: 2.1785\n",
      "Epoch 32/200 Iteration 780| Training loss: 2.1805\n",
      "Epoch 32/200 Iteration 790| Training loss: 2.1989\n",
      "Epoch 32/200 Iteration 800| Training loss: 2.1431\n",
      "Epoch 33/200 Iteration 810| Training loss: 2.1888\n",
      "Epoch 33/200 Iteration 820| Training loss: 2.1639\n",
      "Epoch 34/200 Iteration 830| Training loss: 2.1665\n",
      "Epoch 34/200 Iteration 840| Training loss: 2.1933\n",
      "Epoch 34/200 Iteration 850| Training loss: 2.1121\n",
      "Epoch 35/200 Iteration 860| Training loss: 2.1782\n",
      "Epoch 35/200 Iteration 870| Training loss: 2.1468\n",
      "Epoch 36/200 Iteration 880| Training loss: 2.1479\n",
      "Epoch 36/200 Iteration 890| Training loss: 2.1796\n",
      "Epoch 36/200 Iteration 900| Training loss: 2.1061\n",
      "Epoch 37/200 Iteration 910| Training loss: 2.1701\n",
      "Epoch 37/200 Iteration 920| Training loss: 2.1374\n",
      "Epoch 38/200 Iteration 930| Training loss: 2.1450\n",
      "Epoch 38/200 Iteration 940| Training loss: 2.1587\n",
      "Epoch 38/200 Iteration 950| Training loss: 2.0922\n",
      "Epoch 39/200 Iteration 960| Training loss: 2.1458\n",
      "Epoch 39/200 Iteration 970| Training loss: 2.1211\n",
      "Epoch 40/200 Iteration 980| Training loss: 2.1268\n",
      "Epoch 40/200 Iteration 990| Training loss: 2.1609\n",
      "Epoch 40/200 Iteration 1000| Training loss: 2.0729\n",
      "Epoch 41/200 Iteration 1010| Training loss: 2.1246\n",
      "Epoch 41/200 Iteration 1020| Training loss: 2.1059\n",
      "Epoch 42/200 Iteration 1030| Training loss: 2.1065\n",
      "Epoch 42/200 Iteration 1040| Training loss: 2.1427\n",
      "Epoch 42/200 Iteration 1050| Training loss: 2.0728\n",
      "Epoch 43/200 Iteration 1060| Training loss: 2.1191\n",
      "Epoch 43/200 Iteration 1070| Training loss: 2.1042\n",
      "Epoch 44/200 Iteration 1080| Training loss: 2.0994\n",
      "Epoch 44/200 Iteration 1090| Training loss: 2.1224\n",
      "Epoch 44/200 Iteration 1100| Training loss: 2.0598\n",
      "Epoch 45/200 Iteration 1110| Training loss: 2.1078\n",
      "Epoch 45/200 Iteration 1120| Training loss: 2.0788\n",
      "Epoch 46/200 Iteration 1130| Training loss: 2.0820\n",
      "Epoch 46/200 Iteration 1140| Training loss: 2.1141\n",
      "Epoch 46/200 Iteration 1150| Training loss: 2.0433\n",
      "Epoch 47/200 Iteration 1160| Training loss: 2.0978\n",
      "Epoch 47/200 Iteration 1170| Training loss: 2.0649\n",
      "Epoch 48/200 Iteration 1180| Training loss: 2.0673\n",
      "Epoch 48/200 Iteration 1190| Training loss: 2.1105\n",
      "Epoch 48/200 Iteration 1200| Training loss: 2.0381\n",
      "Epoch 49/200 Iteration 1210| Training loss: 2.0902\n",
      "Epoch 49/200 Iteration 1220| Training loss: 2.0549\n",
      "Epoch 50/200 Iteration 1230| Training loss: 2.0722\n",
      "Epoch 50/200 Iteration 1240| Training loss: 2.0933\n",
      "Epoch 50/200 Iteration 1250| Training loss: 2.0255\n",
      "Epoch 51/200 Iteration 1260| Training loss: 2.0710\n",
      "Epoch 51/200 Iteration 1270| Training loss: 2.0431\n",
      "Epoch 52/200 Iteration 1280| Training loss: 2.0442\n",
      "Epoch 52/200 Iteration 1290| Training loss: 2.0811\n",
      "Epoch 52/200 Iteration 1300| Training loss: 2.0103\n",
      "Epoch 53/200 Iteration 1310| Training loss: 2.0539\n",
      "Epoch 53/200 Iteration 1320| Training loss: 2.0341\n",
      "Epoch 54/200 Iteration 1330| Training loss: 2.0429\n",
      "Epoch 54/200 Iteration 1340| Training loss: 2.0641\n",
      "Epoch 54/200 Iteration 1350| Training loss: 2.0011\n",
      "Epoch 55/200 Iteration 1360| Training loss: 2.0580\n",
      "Epoch 55/200 Iteration 1370| Training loss: 2.0266\n",
      "Epoch 56/200 Iteration 1380| Training loss: 2.0441\n",
      "Epoch 56/200 Iteration 1390| Training loss: 2.0617\n",
      "Epoch 56/200 Iteration 1400| Training loss: 1.9839\n",
      "Epoch 57/200 Iteration 1410| Training loss: 2.0456\n",
      "Epoch 57/200 Iteration 1420| Training loss: 2.0132\n",
      "Epoch 58/200 Iteration 1430| Training loss: 2.0105\n",
      "Epoch 58/200 Iteration 1440| Training loss: 2.0448\n",
      "Epoch 58/200 Iteration 1450| Training loss: 1.9814\n",
      "Epoch 59/200 Iteration 1460| Training loss: 2.0307\n",
      "Epoch 59/200 Iteration 1470| Training loss: 2.0088\n",
      "Epoch 60/200 Iteration 1480| Training loss: 2.0190\n",
      "Epoch 60/200 Iteration 1490| Training loss: 2.0488\n",
      "Epoch 60/200 Iteration 1500| Training loss: 1.9692\n",
      "Epoch 61/200 Iteration 1510| Training loss: 2.0067\n",
      "Epoch 61/200 Iteration 1520| Training loss: 2.0015\n",
      "Epoch 62/200 Iteration 1530| Training loss: 2.0048\n",
      "Epoch 62/200 Iteration 1540| Training loss: 2.0288\n",
      "Epoch 62/200 Iteration 1550| Training loss: 1.9626\n",
      "Epoch 63/200 Iteration 1560| Training loss: 2.0170\n",
      "Epoch 63/200 Iteration 1570| Training loss: 2.0022\n",
      "Epoch 64/200 Iteration 1580| Training loss: 1.9897\n",
      "Epoch 64/200 Iteration 1590| Training loss: 2.0142\n",
      "Epoch 64/200 Iteration 1600| Training loss: 1.9591\n",
      "Epoch 65/200 Iteration 1610| Training loss: 1.9982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200 Iteration 1620| Training loss: 1.9826\n",
      "Epoch 66/200 Iteration 1630| Training loss: 1.9844\n",
      "Epoch 66/200 Iteration 1640| Training loss: 2.0198\n",
      "Epoch 66/200 Iteration 1650| Training loss: 1.9418\n",
      "Epoch 67/200 Iteration 1660| Training loss: 1.9934\n",
      "Epoch 67/200 Iteration 1670| Training loss: 1.9723\n",
      "Epoch 68/200 Iteration 1680| Training loss: 1.9824\n",
      "Epoch 68/200 Iteration 1690| Training loss: 2.0136\n",
      "Epoch 68/200 Iteration 1700| Training loss: 1.9376\n",
      "Epoch 69/200 Iteration 1710| Training loss: 1.9881\n",
      "Epoch 69/200 Iteration 1720| Training loss: 1.9659\n",
      "Epoch 70/200 Iteration 1730| Training loss: 1.9675\n",
      "Epoch 70/200 Iteration 1740| Training loss: 1.9971\n",
      "Epoch 70/200 Iteration 1750| Training loss: 1.9242\n",
      "Epoch 71/200 Iteration 1760| Training loss: 1.9747\n",
      "Epoch 71/200 Iteration 1770| Training loss: 1.9614\n",
      "Epoch 72/200 Iteration 1780| Training loss: 1.9616\n",
      "Epoch 72/200 Iteration 1790| Training loss: 1.9944\n",
      "Epoch 72/200 Iteration 1800| Training loss: 1.9234\n",
      "Epoch 73/200 Iteration 1810| Training loss: 1.9702\n",
      "Epoch 73/200 Iteration 1820| Training loss: 1.9461\n",
      "Epoch 74/200 Iteration 1830| Training loss: 1.9603\n",
      "Epoch 74/200 Iteration 1840| Training loss: 1.9731\n",
      "Epoch 74/200 Iteration 1850| Training loss: 1.9281\n",
      "Epoch 75/200 Iteration 1860| Training loss: 1.9596\n",
      "Epoch 75/200 Iteration 1870| Training loss: 1.9424\n",
      "Epoch 76/200 Iteration 1880| Training loss: 1.9433\n",
      "Epoch 76/200 Iteration 1890| Training loss: 1.9772\n",
      "Epoch 76/200 Iteration 1900| Training loss: 1.9037\n",
      "Epoch 77/200 Iteration 1910| Training loss: 1.9522\n",
      "Epoch 77/200 Iteration 1920| Training loss: 1.9421\n",
      "Epoch 78/200 Iteration 1930| Training loss: 1.9469\n",
      "Epoch 78/200 Iteration 1940| Training loss: 1.9718\n",
      "Epoch 78/200 Iteration 1950| Training loss: 1.9023\n",
      "Epoch 79/200 Iteration 1960| Training loss: 1.9398\n",
      "Epoch 79/200 Iteration 1970| Training loss: 1.9218\n",
      "Epoch 80/200 Iteration 1980| Training loss: 1.9441\n",
      "Epoch 80/200 Iteration 1990| Training loss: 1.9567\n",
      "Epoch 80/200 Iteration 2000| Training loss: 1.8876\n",
      "Epoch 81/200 Iteration 2010| Training loss: 1.9388\n",
      "Epoch 81/200 Iteration 2020| Training loss: 1.9273\n",
      "Epoch 82/200 Iteration 2030| Training loss: 1.9244\n",
      "Epoch 82/200 Iteration 2040| Training loss: 1.9613\n",
      "Epoch 82/200 Iteration 2050| Training loss: 1.8984\n",
      "Epoch 83/200 Iteration 2060| Training loss: 1.9355\n",
      "Epoch 83/200 Iteration 2070| Training loss: 1.9170\n",
      "Epoch 84/200 Iteration 2080| Training loss: 1.9244\n",
      "Epoch 84/200 Iteration 2090| Training loss: 1.9587\n",
      "Epoch 84/200 Iteration 2100| Training loss: 1.8861\n",
      "Epoch 85/200 Iteration 2110| Training loss: 1.9264\n",
      "Epoch 85/200 Iteration 2120| Training loss: 1.9084\n",
      "Epoch 86/200 Iteration 2130| Training loss: 1.9158\n",
      "Epoch 86/200 Iteration 2140| Training loss: 1.9383\n",
      "Epoch 86/200 Iteration 2150| Training loss: 1.8733\n",
      "Epoch 87/200 Iteration 2160| Training loss: 1.9045\n",
      "Epoch 87/200 Iteration 2170| Training loss: 1.9098\n",
      "Epoch 88/200 Iteration 2180| Training loss: 1.9119\n",
      "Epoch 88/200 Iteration 2190| Training loss: 1.9429\n",
      "Epoch 88/200 Iteration 2200| Training loss: 1.8682\n",
      "Epoch 89/200 Iteration 2210| Training loss: 1.9148\n",
      "Epoch 89/200 Iteration 2220| Training loss: 1.8907\n",
      "Epoch 90/200 Iteration 2230| Training loss: 1.9026\n",
      "Epoch 90/200 Iteration 2240| Training loss: 1.9244\n",
      "Epoch 90/200 Iteration 2250| Training loss: 1.8730\n",
      "Epoch 91/200 Iteration 2260| Training loss: 1.9099\n",
      "Epoch 91/200 Iteration 2270| Training loss: 1.8947\n",
      "Epoch 92/200 Iteration 2280| Training loss: 1.9048\n",
      "Epoch 92/200 Iteration 2290| Training loss: 1.9291\n",
      "Epoch 92/200 Iteration 2300| Training loss: 1.8627\n",
      "Epoch 93/200 Iteration 2310| Training loss: 1.8901\n",
      "Epoch 93/200 Iteration 2320| Training loss: 1.8837\n",
      "Epoch 94/200 Iteration 2330| Training loss: 1.8831\n",
      "Epoch 94/200 Iteration 2340| Training loss: 1.9159\n",
      "Epoch 94/200 Iteration 2350| Training loss: 1.8505\n",
      "Epoch 95/200 Iteration 2360| Training loss: 1.8934\n",
      "Epoch 95/200 Iteration 2370| Training loss: 1.8765\n",
      "Epoch 96/200 Iteration 2380| Training loss: 1.8937\n",
      "Epoch 96/200 Iteration 2390| Training loss: 1.9171\n",
      "Epoch 96/200 Iteration 2400| Training loss: 1.8494\n",
      "Epoch 97/200 Iteration 2410| Training loss: 1.8804\n",
      "Epoch 97/200 Iteration 2420| Training loss: 1.8735\n",
      "Epoch 98/200 Iteration 2430| Training loss: 1.8795\n",
      "Epoch 98/200 Iteration 2440| Training loss: 1.9073\n",
      "Epoch 98/200 Iteration 2450| Training loss: 1.8472\n",
      "Epoch 99/200 Iteration 2460| Training loss: 1.8854\n",
      "Epoch 99/200 Iteration 2470| Training loss: 1.8619\n",
      "Epoch 100/200 Iteration 2480| Training loss: 1.8800\n",
      "Epoch 100/200 Iteration 2490| Training loss: 1.9112\n",
      "Epoch 100/200 Iteration 2500| Training loss: 1.8458\n",
      "Epoch 101/200 Iteration 2510| Training loss: 1.8821\n",
      "Epoch 101/200 Iteration 2520| Training loss: 1.8686\n",
      "Epoch 102/200 Iteration 2530| Training loss: 1.8800\n",
      "Epoch 102/200 Iteration 2540| Training loss: 1.8937\n",
      "Epoch 102/200 Iteration 2550| Training loss: 1.8301\n",
      "Epoch 103/200 Iteration 2560| Training loss: 1.8788\n",
      "Epoch 103/200 Iteration 2570| Training loss: 1.8663\n",
      "Epoch 104/200 Iteration 2580| Training loss: 1.8695\n",
      "Epoch 104/200 Iteration 2590| Training loss: 1.8849\n",
      "Epoch 104/200 Iteration 2600| Training loss: 1.8343\n",
      "Epoch 105/200 Iteration 2610| Training loss: 1.8640\n",
      "Epoch 105/200 Iteration 2620| Training loss: 1.8570\n",
      "Epoch 106/200 Iteration 2630| Training loss: 1.8548\n",
      "Epoch 106/200 Iteration 2640| Training loss: 1.8873\n",
      "Epoch 106/200 Iteration 2650| Training loss: 1.8366\n",
      "Epoch 107/200 Iteration 2660| Training loss: 1.8631\n",
      "Epoch 107/200 Iteration 2670| Training loss: 1.8589\n",
      "Epoch 108/200 Iteration 2680| Training loss: 1.8617\n",
      "Epoch 108/200 Iteration 2690| Training loss: 1.8795\n",
      "Epoch 108/200 Iteration 2700| Training loss: 1.8287\n",
      "Epoch 109/200 Iteration 2710| Training loss: 1.8600\n",
      "Epoch 109/200 Iteration 2720| Training loss: 1.8542\n",
      "Epoch 110/200 Iteration 2730| Training loss: 1.8468\n",
      "Epoch 110/200 Iteration 2740| Training loss: 1.8978\n",
      "Epoch 110/200 Iteration 2750| Training loss: 1.8263\n",
      "Epoch 111/200 Iteration 2760| Training loss: 1.8542\n",
      "Epoch 111/200 Iteration 2770| Training loss: 1.8383\n",
      "Epoch 112/200 Iteration 2780| Training loss: 1.8422\n",
      "Epoch 112/200 Iteration 2790| Training loss: 1.8815\n",
      "Epoch 112/200 Iteration 2800| Training loss: 1.8241\n",
      "Epoch 113/200 Iteration 2810| Training loss: 1.8463\n",
      "Epoch 113/200 Iteration 2820| Training loss: 1.8442\n",
      "Epoch 114/200 Iteration 2830| Training loss: 1.8433\n",
      "Epoch 114/200 Iteration 2840| Training loss: 1.8622\n",
      "Epoch 114/200 Iteration 2850| Training loss: 1.8118\n",
      "Epoch 115/200 Iteration 2860| Training loss: 1.8370\n",
      "Epoch 115/200 Iteration 2870| Training loss: 1.8351\n",
      "Epoch 116/200 Iteration 2880| Training loss: 1.8309\n",
      "Epoch 116/200 Iteration 2890| Training loss: 1.8601\n",
      "Epoch 116/200 Iteration 2900| Training loss: 1.8005\n",
      "Epoch 117/200 Iteration 2910| Training loss: 1.8352\n",
      "Epoch 117/200 Iteration 2920| Training loss: 1.8291\n",
      "Epoch 118/200 Iteration 2930| Training loss: 1.8372\n",
      "Epoch 118/200 Iteration 2940| Training loss: 1.8604\n",
      "Epoch 118/200 Iteration 2950| Training loss: 1.8095\n",
      "Epoch 119/200 Iteration 2960| Training loss: 1.8371\n",
      "Epoch 119/200 Iteration 2970| Training loss: 1.8070\n",
      "Epoch 120/200 Iteration 2980| Training loss: 1.8102\n",
      "Epoch 120/200 Iteration 2990| Training loss: 1.8522\n",
      "Epoch 120/200 Iteration 3000| Training loss: 1.7967\n",
      "Epoch 121/200 Iteration 3010| Training loss: 1.8372\n",
      "Epoch 121/200 Iteration 3020| Training loss: 1.8157\n",
      "Epoch 122/200 Iteration 3030| Training loss: 1.8157\n",
      "Epoch 122/200 Iteration 3040| Training loss: 1.8499\n",
      "Epoch 122/200 Iteration 3050| Training loss: 1.7809\n",
      "Epoch 123/200 Iteration 3060| Training loss: 1.8221\n",
      "Epoch 123/200 Iteration 3070| Training loss: 1.8266\n",
      "Epoch 124/200 Iteration 3080| Training loss: 1.8164\n",
      "Epoch 124/200 Iteration 3090| Training loss: 1.8480\n",
      "Epoch 124/200 Iteration 3100| Training loss: 1.7795\n",
      "Epoch 125/200 Iteration 3110| Training loss: 1.8229\n",
      "Epoch 125/200 Iteration 3120| Training loss: 1.8110\n",
      "Epoch 126/200 Iteration 3130| Training loss: 1.8261\n",
      "Epoch 126/200 Iteration 3140| Training loss: 1.8364\n",
      "Epoch 126/200 Iteration 3150| Training loss: 1.7794\n",
      "Epoch 127/200 Iteration 3160| Training loss: 1.8084\n",
      "Epoch 127/200 Iteration 3170| Training loss: 1.8007\n",
      "Epoch 128/200 Iteration 3180| Training loss: 1.8096\n",
      "Epoch 128/200 Iteration 3190| Training loss: 1.8445\n",
      "Epoch 128/200 Iteration 3200| Training loss: 1.7696\n",
      "Epoch 129/200 Iteration 3210| Training loss: 1.8166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/200 Iteration 3220| Training loss: 1.8148\n",
      "Epoch 130/200 Iteration 3230| Training loss: 1.7982\n",
      "Epoch 130/200 Iteration 3240| Training loss: 1.8216\n",
      "Epoch 130/200 Iteration 3250| Training loss: 1.7708\n",
      "Epoch 131/200 Iteration 3260| Training loss: 1.8126\n",
      "Epoch 131/200 Iteration 3270| Training loss: 1.8006\n",
      "Epoch 132/200 Iteration 3280| Training loss: 1.8143\n",
      "Epoch 132/200 Iteration 3290| Training loss: 1.8326\n",
      "Epoch 132/200 Iteration 3300| Training loss: 1.7723\n",
      "Epoch 133/200 Iteration 3310| Training loss: 1.8107\n",
      "Epoch 133/200 Iteration 3320| Training loss: 1.7905\n",
      "Epoch 134/200 Iteration 3330| Training loss: 1.8100\n",
      "Epoch 134/200 Iteration 3340| Training loss: 1.8359\n",
      "Epoch 134/200 Iteration 3350| Training loss: 1.7782\n",
      "Epoch 135/200 Iteration 3360| Training loss: 1.8100\n",
      "Epoch 135/200 Iteration 3370| Training loss: 1.7986\n",
      "Epoch 136/200 Iteration 3380| Training loss: 1.7978\n",
      "Epoch 136/200 Iteration 3390| Training loss: 1.8192\n",
      "Epoch 136/200 Iteration 3400| Training loss: 1.7602\n",
      "Epoch 137/200 Iteration 3410| Training loss: 1.8058\n",
      "Epoch 137/200 Iteration 3420| Training loss: 1.7921\n",
      "Epoch 138/200 Iteration 3430| Training loss: 1.7957\n",
      "Epoch 138/200 Iteration 3440| Training loss: 1.8217\n",
      "Epoch 138/200 Iteration 3450| Training loss: 1.7570\n",
      "Epoch 139/200 Iteration 3460| Training loss: 1.7928\n",
      "Epoch 139/200 Iteration 3470| Training loss: 1.7894\n",
      "Epoch 140/200 Iteration 3480| Training loss: 1.7889\n",
      "Epoch 140/200 Iteration 3490| Training loss: 1.8104\n",
      "Epoch 140/200 Iteration 3500| Training loss: 1.7610\n",
      "Epoch 141/200 Iteration 3510| Training loss: 1.7959\n",
      "Epoch 141/200 Iteration 3520| Training loss: 1.7798\n",
      "Epoch 142/200 Iteration 3530| Training loss: 1.7789\n",
      "Epoch 142/200 Iteration 3540| Training loss: 1.8132\n",
      "Epoch 142/200 Iteration 3550| Training loss: 1.7414\n",
      "Epoch 143/200 Iteration 3560| Training loss: 1.7912\n",
      "Epoch 143/200 Iteration 3570| Training loss: 1.7721\n",
      "Epoch 144/200 Iteration 3580| Training loss: 1.7841\n",
      "Epoch 144/200 Iteration 3590| Training loss: 1.8087\n",
      "Epoch 144/200 Iteration 3600| Training loss: 1.7520\n",
      "Epoch 145/200 Iteration 3610| Training loss: 1.7776\n",
      "Epoch 145/200 Iteration 3620| Training loss: 1.7732\n",
      "Epoch 146/200 Iteration 3630| Training loss: 1.7827\n",
      "Epoch 146/200 Iteration 3640| Training loss: 1.8022\n",
      "Epoch 146/200 Iteration 3650| Training loss: 1.7462\n",
      "Epoch 147/200 Iteration 3660| Training loss: 1.7778\n",
      "Epoch 147/200 Iteration 3670| Training loss: 1.7719\n",
      "Epoch 148/200 Iteration 3680| Training loss: 1.7685\n",
      "Epoch 148/200 Iteration 3690| Training loss: 1.7978\n",
      "Epoch 148/200 Iteration 3700| Training loss: 1.7414\n",
      "Epoch 149/200 Iteration 3710| Training loss: 1.7756\n",
      "Epoch 149/200 Iteration 3720| Training loss: 1.7563\n",
      "Epoch 150/200 Iteration 3730| Training loss: 1.7709\n",
      "Epoch 150/200 Iteration 3740| Training loss: 1.7932\n",
      "Epoch 150/200 Iteration 3750| Training loss: 1.7313\n",
      "Epoch 151/200 Iteration 3760| Training loss: 1.7756\n",
      "Epoch 151/200 Iteration 3770| Training loss: 1.7686\n",
      "Epoch 152/200 Iteration 3780| Training loss: 1.7677\n",
      "Epoch 152/200 Iteration 3790| Training loss: 1.7867\n",
      "Epoch 152/200 Iteration 3800| Training loss: 1.7312\n",
      "Epoch 153/200 Iteration 3810| Training loss: 1.7796\n",
      "Epoch 153/200 Iteration 3820| Training loss: 1.7499\n",
      "Epoch 154/200 Iteration 3830| Training loss: 1.7516\n",
      "Epoch 154/200 Iteration 3840| Training loss: 1.7822\n",
      "Epoch 154/200 Iteration 3850| Training loss: 1.7443\n",
      "Epoch 155/200 Iteration 3860| Training loss: 1.7609\n",
      "Epoch 155/200 Iteration 3870| Training loss: 1.7597\n",
      "Epoch 156/200 Iteration 3880| Training loss: 1.7600\n",
      "Epoch 156/200 Iteration 3890| Training loss: 1.8026\n",
      "Epoch 156/200 Iteration 3900| Training loss: 1.7501\n",
      "Epoch 157/200 Iteration 3910| Training loss: 1.7787\n",
      "Epoch 157/200 Iteration 3920| Training loss: 1.7476\n",
      "Epoch 158/200 Iteration 3930| Training loss: 1.7620\n",
      "Epoch 158/200 Iteration 3940| Training loss: 1.7846\n",
      "Epoch 158/200 Iteration 3950| Training loss: 1.7352\n",
      "Epoch 159/200 Iteration 3960| Training loss: 1.7499\n",
      "Epoch 159/200 Iteration 3970| Training loss: 1.7462\n",
      "Epoch 160/200 Iteration 3980| Training loss: 1.7528\n",
      "Epoch 160/200 Iteration 3990| Training loss: 1.7733\n",
      "Epoch 160/200 Iteration 4000| Training loss: 1.7256\n",
      "Epoch 161/200 Iteration 4010| Training loss: 1.7541\n",
      "Epoch 161/200 Iteration 4020| Training loss: 1.7406\n",
      "Epoch 162/200 Iteration 4030| Training loss: 1.7541\n",
      "Epoch 162/200 Iteration 4040| Training loss: 1.7773\n",
      "Epoch 162/200 Iteration 4050| Training loss: 1.7206\n",
      "Epoch 163/200 Iteration 4060| Training loss: 1.7527\n",
      "Epoch 163/200 Iteration 4070| Training loss: 1.7467\n",
      "Epoch 164/200 Iteration 4080| Training loss: 1.7465\n",
      "Epoch 164/200 Iteration 4090| Training loss: 1.7679\n",
      "Epoch 164/200 Iteration 4100| Training loss: 1.7233\n",
      "Epoch 165/200 Iteration 4110| Training loss: 1.7464\n",
      "Epoch 165/200 Iteration 4120| Training loss: 1.7492\n",
      "Epoch 166/200 Iteration 4130| Training loss: 1.7277\n",
      "Epoch 166/200 Iteration 4140| Training loss: 1.7664\n",
      "Epoch 166/200 Iteration 4150| Training loss: 1.7170\n",
      "Epoch 167/200 Iteration 4160| Training loss: 1.7553\n",
      "Epoch 167/200 Iteration 4170| Training loss: 1.7495\n",
      "Epoch 168/200 Iteration 4180| Training loss: 1.7312\n",
      "Epoch 168/200 Iteration 4190| Training loss: 1.7632\n",
      "Epoch 168/200 Iteration 4200| Training loss: 1.7180\n",
      "Epoch 169/200 Iteration 4210| Training loss: 1.7328\n",
      "Epoch 169/200 Iteration 4220| Training loss: 1.7378\n",
      "Epoch 170/200 Iteration 4230| Training loss: 1.7268\n",
      "Epoch 170/200 Iteration 4240| Training loss: 1.7619\n",
      "Epoch 170/200 Iteration 4250| Training loss: 1.7124\n",
      "Epoch 171/200 Iteration 4260| Training loss: 1.7373\n",
      "Epoch 171/200 Iteration 4270| Training loss: 1.7250\n",
      "Epoch 172/200 Iteration 4280| Training loss: 1.7302\n",
      "Epoch 172/200 Iteration 4290| Training loss: 1.7762\n",
      "Epoch 172/200 Iteration 4300| Training loss: 1.7021\n",
      "Epoch 173/200 Iteration 4310| Training loss: 1.7354\n",
      "Epoch 173/200 Iteration 4320| Training loss: 1.7277\n",
      "Epoch 174/200 Iteration 4330| Training loss: 1.7313\n",
      "Epoch 174/200 Iteration 4340| Training loss: 1.7596\n",
      "Epoch 174/200 Iteration 4350| Training loss: 1.7039\n",
      "Epoch 175/200 Iteration 4360| Training loss: 1.7370\n",
      "Epoch 175/200 Iteration 4370| Training loss: 1.7260\n",
      "Epoch 176/200 Iteration 4380| Training loss: 1.7323\n",
      "Epoch 176/200 Iteration 4390| Training loss: 1.7610\n",
      "Epoch 176/200 Iteration 4400| Training loss: 1.7082\n",
      "Epoch 177/200 Iteration 4410| Training loss: 1.7349\n",
      "Epoch 177/200 Iteration 4420| Training loss: 1.7199\n",
      "Epoch 178/200 Iteration 4430| Training loss: 1.7239\n",
      "Epoch 178/200 Iteration 4440| Training loss: 1.7523\n",
      "Epoch 178/200 Iteration 4450| Training loss: 1.7041\n",
      "Epoch 179/200 Iteration 4460| Training loss: 1.7137\n",
      "Epoch 179/200 Iteration 4470| Training loss: 1.7231\n",
      "Epoch 180/200 Iteration 4480| Training loss: 1.7287\n",
      "Epoch 180/200 Iteration 4490| Training loss: 1.7521\n",
      "Epoch 180/200 Iteration 4500| Training loss: 1.7002\n",
      "Epoch 181/200 Iteration 4510| Training loss: 1.7207\n",
      "Epoch 181/200 Iteration 4520| Training loss: 1.7106\n",
      "Epoch 182/200 Iteration 4530| Training loss: 1.7097\n",
      "Epoch 182/200 Iteration 4540| Training loss: 1.7605\n",
      "Epoch 182/200 Iteration 4550| Training loss: 1.7051\n",
      "Epoch 183/200 Iteration 4560| Training loss: 1.7146\n",
      "Epoch 183/200 Iteration 4570| Training loss: 1.7092\n",
      "Epoch 184/200 Iteration 4580| Training loss: 1.7209\n",
      "Epoch 184/200 Iteration 4590| Training loss: 1.7441\n",
      "Epoch 184/200 Iteration 4600| Training loss: 1.6950\n",
      "Epoch 185/200 Iteration 4610| Training loss: 1.7192\n",
      "Epoch 185/200 Iteration 4620| Training loss: 1.7101\n",
      "Epoch 186/200 Iteration 4630| Training loss: 1.7205\n",
      "Epoch 186/200 Iteration 4640| Training loss: 1.7354\n",
      "Epoch 186/200 Iteration 4650| Training loss: 1.6833\n",
      "Epoch 187/200 Iteration 4660| Training loss: 1.7178\n",
      "Epoch 187/200 Iteration 4670| Training loss: 1.7091\n",
      "Epoch 188/200 Iteration 4680| Training loss: 1.7138\n",
      "Epoch 188/200 Iteration 4690| Training loss: 1.7358\n",
      "Epoch 188/200 Iteration 4700| Training loss: 1.6863\n",
      "Epoch 189/200 Iteration 4710| Training loss: 1.7077\n",
      "Epoch 189/200 Iteration 4720| Training loss: 1.7070\n",
      "Epoch 190/200 Iteration 4730| Training loss: 1.7105\n",
      "Epoch 190/200 Iteration 4740| Training loss: 1.7387\n",
      "Epoch 190/200 Iteration 4750| Training loss: 1.6840\n",
      "Epoch 191/200 Iteration 4760| Training loss: 1.7138\n",
      "Epoch 191/200 Iteration 4770| Training loss: 1.7121\n",
      "Epoch 192/200 Iteration 4780| Training loss: 1.7040\n",
      "Epoch 192/200 Iteration 4790| Training loss: 1.7362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/200 Iteration 4800| Training loss: 1.6813\n",
      "Epoch 193/200 Iteration 4810| Training loss: 1.7154\n",
      "Epoch 193/200 Iteration 4820| Training loss: 1.6998\n",
      "Epoch 194/200 Iteration 4830| Training loss: 1.6819\n",
      "Epoch 194/200 Iteration 4840| Training loss: 1.7234\n",
      "Epoch 194/200 Iteration 4850| Training loss: 1.6906\n",
      "Epoch 195/200 Iteration 4860| Training loss: 1.7096\n",
      "Epoch 195/200 Iteration 4870| Training loss: 1.7077\n",
      "Epoch 196/200 Iteration 4880| Training loss: 1.7095\n",
      "Epoch 196/200 Iteration 4890| Training loss: 1.7164\n",
      "Epoch 196/200 Iteration 4900| Training loss: 1.6662\n",
      "Epoch 197/200 Iteration 4910| Training loss: 1.7006\n",
      "Epoch 197/200 Iteration 4920| Training loss: 1.6880\n",
      "Epoch 198/200 Iteration 4930| Training loss: 1.6997\n",
      "Epoch 198/200 Iteration 4940| Training loss: 1.7251\n",
      "Epoch 198/200 Iteration 4950| Training loss: 1.6611\n",
      "Epoch 199/200 Iteration 4960| Training loss: 1.6994\n",
      "Epoch 199/200 Iteration 4970| Training loss: 1.6911\n",
      "Epoch 200/200 Iteration 4980| Training loss: 1.6927\n",
      "Epoch 200/200 Iteration 4990| Training loss: 1.7208\n",
      "Epoch 200/200 Iteration 5000| Training loss: 1.6690\n"
     ]
    }
   ],
   "source": [
    "## run for 200 epochs\n",
    "batch_size = 64\n",
    "num_steps = 100\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y,\n",
    "          num_epochs=200,\n",
    "          ckpt_dir='./model-200/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(1, 65), dtype=float32)\n",
      "The mar they are a was,\n",
      "With ho atshould the soues it selfe with the Sirs, these\n",
      "Tame tore arien the bad to hemremathith of,\n",
      "Thene our Stinges of it we thanke stell his ton,\n",
      "As that his fores it whise that well will mere her\n",
      "Asce the sayed on to that\n",
      "\n",
      "   Ham. No makn you an ward may.\n",
      "Theseering on in shampore of might, in a to butsere\n",
      "though night at inthe hast to bound the Corning inderte\n",
      "To this mant is forly and that whet in this arande\n",
      "\n",
      "   King. Thoundsse, but and woll ta seaded at on tin see,\n",
      "At\n"
     ]
    }
   ],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir='./model-200/',\n",
    "                 output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
